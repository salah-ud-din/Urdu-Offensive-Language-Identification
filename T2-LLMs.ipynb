{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzvqAc0ozC37"
   },
   "source": [
    "PyTorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLwksgzY00tj",
    "outputId": "1de37261-8f67-472a-a741-89b956fc68ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "file_path =  '/content/drive/MyDrive/T2-Code/Offensive-24K-T2.xlsx'\n",
    "file_path =  '/content/drive/MyDrive/Dataset-v2/Offensive-24K-T1.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "   Unnamed: 0                                              Tweet  Tag\n",
      "0           0  USER دو بے نسلئیے ، حرامخور منافق مل رہے ہیں پ...    0\n",
      "1           3                   USER گھٹیا انسان دنیا ہی چھوڑ دو    1\n",
      "2          11  USER PMLN میں آپ کے بارے میں میری بہتر راۓ تھی...    1\n",
      "3          15  USER اسپین کی ٹیم بھی پاکستان کی کرکٹ ٹیم کی ط...    0\n",
      "4          20  ہمیں تو آج تک سمجھ نہیں آئی کہ کم عقل عیسائی ح...    1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8758 entries, 0 to 8757\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8758 non-null   int64 \n",
      " 1   Tweet       8758 non-null   object\n",
      " 2   Tag         8758 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "None\n",
      "Index(['Unnamed: 0', 'Tweet', 'Tag'], dtype='object') (8758, 3)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc, os,time,numpy as np, pandas as pd, datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, MT5EncoderModel, logging\n",
    "\n",
    "# ------------ GPU Setup ------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':torch.cuda.empty_cache()\n",
    "\n",
    "# ---------- Hyperparameters ---------\n",
    "MAX_LEN = 128\n",
    "EMBED_SIZE = 768\n",
    "BERT_TRAINABLE = True\n",
    "DRPT = 0.4\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 6e-5\n",
    "BATCH = 25  # reduced to lower GPU memory usage\n",
    "NEPOCHS = 20\n",
    "PATIENCE = 4\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1\n",
    "\n",
    "modelname = 'hfFineTuneMT5'\n",
    "modelpath = os.path.join('.', 'Saved Models', modelname)\n",
    "modelresults = os.path.join('.', 'Model Results')\n",
    "modelsummaries = os.path.join('.', 'Model - Summaries-Figures')\n",
    "for d in [modelpath, modelresults, modelsummaries]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ------------ Utils ----------------\n",
    "def hms_string(sec):\n",
    "    h = int(sec // 3600)\n",
    "    m = int((sec % 3600) // 60)\n",
    "    s = sec % 60\n",
    "    return f\"{h} hrs {m:02d} mins {s:05.2f} secs\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# -------- Dataset Class -----------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# -------- Model Definition --------\n",
    "class MT5Classifier(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=DRPT):\n",
    "        super().__init__()\n",
    "        self.bert = MT5EncoderModel.from_pretrained(model_name)\n",
    "        if not BERT_TRAINABLE:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.ELU()\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.out.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pool = out.last_hidden_state[:, 0]\n",
    "        x = self.dropout(pool)\n",
    "        x = self.act(self.fc1(x))\n",
    "        return self.sig(self.out(x)).squeeze(-1)\n",
    "\n",
    "# --- Threshold Optimization -------\n",
    "def optimize_threshold(y_true, y_probs):\n",
    "    best, best_t = 0, .5\n",
    "    for t in np.arange(0.1, 0.9, 0.001):\n",
    "        f = f1_score(y_true, (y_probs >= t).astype(int))\n",
    "        if f > best:\n",
    "            best, best_t = f, t\n",
    "    return best_t\n",
    "# --------- Data Loading ------------\n",
    "########################################################\n",
    "def WriteResutls(reports):\n",
    "  unt0 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  tin1 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  macroavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  weightedavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  accu = []\n",
    "  for report in reports:\n",
    "    for k,v in report.items():\n",
    "      if 'UNT' in k:\n",
    "        unt0['precision'].append(v['precision'])\n",
    "        unt0['recall'].append(v['recall'])\n",
    "        unt0['f1-score'].append(v['f1-score'])\n",
    "      elif 'TIN' in k:\n",
    "        tin1['precision'].append(v['precision'])\n",
    "        tin1['recall'].append(v['recall'])\n",
    "        tin1['f1-score'].append(v['f1-score'])\n",
    "      elif 'macro avg' in k:\n",
    "        macroavg['precision'].append(v['precision'])\n",
    "        macroavg['recall'].append(v['recall'])\n",
    "        macroavg['f1-score'].append(v['f1-score'])\n",
    "      elif 'weighted avg' in k:\n",
    "        weightedavg['precision'].append(v['precision'])\n",
    "        weightedavg['recall'].append(v['recall'])\n",
    "        weightedavg['f1-score'].append(v['f1-score'])\n",
    "      else:\n",
    "        accu.append(v)\n",
    "\n",
    "  print('Accuracy:',np.mean(accu))\n",
    "  print(\"\")\n",
    "  print('UNT 0 Precision:',np.mean(unt0['precision']))\n",
    "  print('UNT 0 Recall:',np.mean(unt0['recall']))\n",
    "  print('UNT 0 F1-Score:',np.mean(unt0['f1-score']))\n",
    "  print(\"\")\n",
    "  print('TIN 1 Precision:',np.mean(tin1['precision']))\n",
    "  print('TIN 1 Recall:',np.mean(tin1['recall']))\n",
    "  print('TIN 1 F1-Score:',np.mean(tin1['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Weighted Avg Precision:',np.mean(weightedavg['precision']))\n",
    "  print('Weighted Avg Recall:',np.mean(weightedavg['recall']))\n",
    "  print('Weighted Avg F1-Score:',np.mean(weightedavg['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Macro  Precision:',np.mean(macroavg['precision']))\n",
    "  print('Macro  Recall:',np.mean(macroavg['recall']))\n",
    "  print('Macro  F1-Score:',np.mean(macroavg['f1-score']))\n",
    "\n",
    "\n",
    "  file = open( result_path, mode='a' )\n",
    "  file.write( modelname+ ' ( '+ datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +') \\n' )\n",
    "  file.write( 'Accuracy:'+str(np.mean(accu))+'\\n' )\n",
    "  file.write('UNT 0 Precision:'+str(np.mean(unt0['precision']))+'\\n' )\n",
    "  file.write('UNT 0 Recall:'+str(np.mean(unt0['recall']))+'\\n' )\n",
    "  file.write('UNT 0 F1-Score:'+str(np.mean(unt0['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('TIN 1 Precision:'+str(np.mean(tin1['precision']))+'\\n' )\n",
    "  file.write('TIN 1 Recall:'+str(np.mean(tin1['recall']))+'\\n' )\n",
    "  file.write('TIN 1 F1-Score:'+str(np.mean(tin1['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Weighted Avg Precision:'+str(np.mean(weightedavg['precision']))+'\\n' )\n",
    "  file.write('Weighted Avg Recall:'+str(np.mean(weightedavg['recall']))+'\\n' )\n",
    "  file.write('Weighted Avg F1-Score:'+str(np.mean(weightedavg['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Macro  Precision:'+str(np.mean(macroavg['precision']))+'\\n' )\n",
    "  file.write('Macro  Recall:'+str(np.mean(macroavg['recall']))+'\\n' )\n",
    "  file.write('Macro  F1-Score:'+str(np.mean(macroavg['f1-score']))+'\\n' )\n",
    "  file.close()\n",
    "  print(\"Done\")\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# -------------- Data Loading ---------------\n",
    "result_path =  r'C:\\Users\\mojua\\Desktop\\DL-Code\\T2-Classification-Result.csv'\n",
    "file_path = r'C:\\Users\\mojua\\Desktop\\DL-Code\\Dataset\\Offensive-24K-T2.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.columns, df.shape)\n",
    "gc.collect()\n",
    "\n",
    "xcolumn = 'Tweet'\n",
    "ycolumn = 'Tag'\n",
    "\n",
    "# --------- Tokenizer Setup --------\n",
    "bertmodelname = 'google/mt5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bertmodelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 12:02 PM\n",
      "Fold1 Ep1: Val MacroF1=0.4419 Pat=0\n",
      "Fold1 Ep2: Val MacroF1=0.4377 Pat=1\n",
      "Fold1 Ep3: Val MacroF1=0.4334 Pat=2\n",
      "Fold1 Ep4: Val MacroF1=0.4377 Pat=3\n",
      "Fold1 Ep5: Val MacroF1=0.4377 Pat=4\n",
      "Stopping early at epoch 5\n",
      "Fold1 done at 12:06 PM\n",
      "Fold2 Ep1: Val MacroF1=0.4334 Pat=0\n",
      "Fold2 Ep2: Val MacroF1=0.5017 Pat=0\n",
      "Fold2 Ep3: Val MacroF1=0.5245 Pat=0\n",
      "Fold2 Ep4: Val MacroF1=0.5936 Pat=0\n",
      "Fold2 Ep5: Val MacroF1=0.5851 Pat=1\n",
      "Fold2 Ep6: Val MacroF1=0.5903 Pat=2\n",
      "Fold2 Ep7: Val MacroF1=0.5903 Pat=3\n",
      "Fold2 Ep8: Val MacroF1=0.5903 Pat=4\n",
      "Stopping early at epoch 8\n",
      "Fold2 done at 12:13 PM\n",
      "Fold3 Ep1: Val MacroF1=0.4384 Pat=0\n",
      "Fold3 Ep2: Val MacroF1=0.5428 Pat=0\n",
      "Fold3 Ep3: Val MacroF1=0.5607 Pat=0\n",
      "Fold3 Ep4: Val MacroF1=0.5952 Pat=0\n",
      "Fold3 Ep5: Val MacroF1=0.5973 Pat=0\n",
      "Fold3 Ep6: Val MacroF1=0.5986 Pat=0\n",
      "Fold3 Ep7: Val MacroF1=0.5938 Pat=1\n",
      "Fold3 Ep8: Val MacroF1=0.6021 Pat=0\n",
      "Fold3 Ep9: Val MacroF1=0.6707 Pat=0\n",
      "Fold3 Ep10: Val MacroF1=0.6089 Pat=1\n",
      "Fold3 Ep11: Val MacroF1=0.6072 Pat=2\n",
      "Fold3 Ep12: Val MacroF1=0.6089 Pat=3\n",
      "Fold3 Ep13: Val MacroF1=0.6089 Pat=4\n",
      "Stopping early at epoch 13\n",
      "Fold3 done at 12:26 PM\n",
      "Fold4 Ep1: Val MacroF1=0.4378 Pat=0\n",
      "Fold4 Ep2: Val MacroF1=0.4458 Pat=0\n",
      "Fold4 Ep3: Val MacroF1=0.5420 Pat=0\n",
      "Fold4 Ep4: Val MacroF1=0.5709 Pat=0\n",
      "Fold4 Ep5: Val MacroF1=0.5724 Pat=0\n",
      "Fold4 Ep6: Val MacroF1=0.5786 Pat=0\n",
      "Fold4 Ep7: Val MacroF1=0.5655 Pat=1\n",
      "Fold4 Ep8: Val MacroF1=0.5608 Pat=2\n",
      "Fold4 Ep9: Val MacroF1=0.5615 Pat=3\n",
      "Fold4 Ep10: Val MacroF1=0.5615 Pat=4\n",
      "Stopping early at epoch 10\n",
      "Fold4 done at 12:36 PM\n",
      "Fold5 Ep1: Val MacroF1=0.4384 Pat=0\n",
      "Fold5 Ep2: Val MacroF1=0.4511 Pat=0\n",
      "Fold5 Ep3: Val MacroF1=0.5487 Pat=0\n",
      "Fold5 Ep4: Val MacroF1=0.5489 Pat=0\n",
      "Fold5 Ep5: Val MacroF1=0.6007 Pat=0\n",
      "Fold5 Ep6: Val MacroF1=0.5683 Pat=1\n",
      "Fold5 Ep7: Val MacroF1=0.5894 Pat=2\n",
      "Fold5 Ep8: Val MacroF1=0.5886 Pat=3\n",
      "Fold5 Ep9: Val MacroF1=0.5863 Pat=4\n",
      "Stopping early at epoch 9\n",
      "Fold5 done at 12:44 PM\n",
      "Total runtime: 0 hrs 42 mins 33.36 secs\n",
      "Accuracy: 0.7716381897884836\n",
      "\n",
      "UNT 0 Precision: 0.5342756539235413\n",
      "UNT 0 Recall: 0.16498267898383373\n",
      "UNT 0 F1-Score: 0.247424729382753\n",
      "\n",
      "TIN 1 Precision: 0.7804792505499669\n",
      "TIN 1 Recall: 0.9707326445173322\n",
      "TIN 1 F1-Score: 0.8649048017445466\n",
      "\n",
      "Weighted Avg Precision: 0.7196418337797226\n",
      "Weighted Avg Recall: 0.7716381897884836\n",
      "Weighted Avg F1-Score: 0.7123290585433389\n",
      "\n",
      "Macro  Precision: 0.6573774522367541\n",
      "Macro  Recall: 0.5678576617505829\n",
      "Macro  F1-Score: 0.5561647655636499\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# --- K-Fold Training -------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# metrics storage\n",
    "valaccuracy, valprecision, valrecall, valf1 = [], [], [], []\n",
    "testaccuracy, testprecision, testrecall, testf1 = [], [], [], []\n",
    "val_f1_macro, test_f1_macro = [], []\n",
    "\n",
    "start = time.time()\n",
    "print('Start:', time.strftime(\"%I:%M %p\"))\n",
    "reports = []\n",
    "for fold, (tr, te) in enumerate(skf.split(df[xcolumn], df[ycolumn]), 1):\n",
    "    # split\n",
    "    X_tr = df.loc[tr, xcolumn].tolist()\n",
    "    y_tr = df.loc[tr, ycolumn].values\n",
    "    X_te = df.loc[te, xcolumn].tolist()\n",
    "    y_te = df.loc[te, ycolumn].values\n",
    "\n",
    "    # train/val split\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split( X_tr, y_tr, test_size=0.15, random_state=0)\n",
    "\n",
    "    # encodings\n",
    "    enc_tr = tokenizer(X_tr, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_val = tokenizer( X_val, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_te = tokenizer(X_te, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    # datasets\n",
    "    ds_tr = TextDataset(enc_tr, y_tr)\n",
    "    ds_val = TextDataset(enc_val, y_val)\n",
    "    ds_te = TextDataset(enc_te, y_te)\n",
    "\n",
    "    # Updated DataLoaders with pinned memory\n",
    "    ld_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    ld_val = DataLoader(ds_val, batch_size=BATCH, num_workers=0, pin_memory=True)\n",
    "    ld_te = DataLoader(ds_te, batch_size=BATCH, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # model, loss, optimizer\n",
    "    model = MT5Classifier(bertmodelname).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "\n",
    "    best_val, patience = 0, 0\n",
    "\n",
    "    for epoch in range(NEPOCHS):\n",
    "        model.train()\n",
    "        for b in ld_tr:\n",
    "            optimizer.zero_grad()\n",
    "            ids = b['input_ids'].to(device, non_blocking=True)\n",
    "            masks = b['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = b['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            probs = model(ids, masks)\n",
    "            loss = criterion(probs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # immediate cleanup\n",
    "            del ids, masks, labels, probs, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        vp, vt = [], []\n",
    "        with torch.no_grad():\n",
    "            for b in ld_val:\n",
    "                outs = model(\n",
    "                    b['input_ids'].to(device),\n",
    "                    b['attention_mask'].to(device)\n",
    "                ).cpu().numpy()\n",
    "                vp.extend(outs)\n",
    "                vt.extend(b['labels'].numpy())\n",
    "\n",
    "        thr = optimize_threshold(np.array(vt), np.array(vp))\n",
    "        vpred = (np.array(vp) >= thr).astype(int)\n",
    "\n",
    "        # compute validation metrics\n",
    "        vm = f1_score(vt, vpred, average='macro')\n",
    "       \n",
    "        # save best\n",
    "        if vm > best_val:\n",
    "            best_val, patience = vm, 0\n",
    "            save_path = os.path.join(modelpath, f\"{modelname}_fold{fold}.pt\")\n",
    "            cpu_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(cpu_state, save_path, _use_new_zipfile_serialization=False)\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        # lr decay\n",
    "        if DECAY and patience % DECAY_AFTER == 0 and patience != 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= DECAY_RATE\n",
    "\n",
    "        print(f\"Fold{fold} Ep{epoch+1}: Val MacroF1={vm:.4f} Pat={patience}\")\n",
    "        if patience >= PATIENCE:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # load best model & test\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(modelpath, f\"{modelname}_fold{fold}.pt\"))\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # evaluate on val & test\n",
    "    vp, vt, tp, tt = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for b in ld_val:\n",
    "            vp.extend(model(\n",
    "                b['input_ids'].to(device),\n",
    "                b['attention_mask'].to(device)\n",
    "            ).cpu().numpy())\n",
    "            vt.extend(b['labels'].numpy())\n",
    "        for b in ld_te:\n",
    "            tp.extend(model(\n",
    "                b['input_ids'].to(device),\n",
    "                b['attention_mask'].to(device)\n",
    "            ).cpu().numpy())\n",
    "            tt.extend(b['labels'].numpy())\n",
    "\n",
    "    thr = optimize_threshold(np.array(vt), np.array(vp))\n",
    "    vpred = (np.array(vp) >= thr).astype(int)\n",
    "    tpred = (np.array(tp) >= thr).astype(int)\n",
    "\n",
    "    # compute test metrics\n",
    "    ta = accuracy_score(tt, tpred)\n",
    "    tp_ = precision_score(tt, tpred)\n",
    "    tr = recall_score(tt, tpred)\n",
    "    tf = f1_score(tt, tpred)\n",
    "    tm = f1_score(tt, tpred, average='macro')\n",
    "    tpc = precision_score(tt, tpred, average=None)\n",
    "    trc = recall_score(tt, tpred, average=None)\n",
    "    tfc = f1_score(tt, tpred, average=None)\n",
    "    reports.append(classification_report( tt, tpred, output_dict=True, zero_division=0, target_names=['UNT 0', 'TIN 1'])) \n",
    "   \n",
    "    print(f\"Fold{fold} done at {time.strftime('%I:%M %p')}\")\n",
    "\n",
    "    # fold-level cleanup\n",
    "    del model, optimizer, criterion, ld_tr, ld_val, ld_te, ds_tr, ds_val, ds_te\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Total runtime: {hms_string(time.time()-start)}\")\n",
    "WriteResutls(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "   Unnamed: 0                                              Tweet  Tag\n",
      "0           0  USER دو بے نسلئیے ، حرامخور منافق مل رہے ہیں پ...    0\n",
      "1           3                   USER گھٹیا انسان دنیا ہی چھوڑ دو    1\n",
      "2          11  USER PMLN میں آپ کے بارے میں میری بہتر راۓ تھی...    1\n",
      "3          15  USER اسپین کی ٹیم بھی پاکستان کی کرکٹ ٹیم کی ط...    0\n",
      "4          20  ہمیں تو آج تک سمجھ نہیں آئی کہ کم عقل عیسائی ح...    1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8758 entries, 0 to 8757\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8758 non-null   int64 \n",
      " 1   Tweet       8758 non-null   object\n",
      " 2   Tag         8758 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "None\n",
      "Index(['Unnamed: 0', 'Tweet', 'Tag'], dtype='object') (8758, 3)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc, os,time, numpy as np,pandas as pd, datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, logging\n",
    "# ------------- GPU Setup ------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "# ---------- Hyperparameters ---------\n",
    "MAX_LEN = 128\n",
    "EMBED_SIZE = 768\n",
    "BERT_TRAINABLE = True\n",
    "DRPT = 0.4\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 9e-6\n",
    "BATCH = 32\n",
    "NEPOCHS = 20\n",
    "PATIENCE = 4\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1\n",
    "\n",
    "modelname = 'hfFineTuneMuril'\n",
    "modelpath = os.path.join('.', 'Saved Models', modelname)\n",
    "modelresults = os.path.join('.', 'Model Results')\n",
    "modelsummaries = os.path.join('.', 'Model - Summaries-Figures')\n",
    "for d in [modelpath, modelresults, modelsummaries]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ---------- Utils ------------------\n",
    "def hms_string(sec):\n",
    "    h = int(sec // 3600)\n",
    "    m = int((sec % 3600) // 60)\n",
    "    s = sec % 60\n",
    "    return f\"{h} hrs {m:02d} mins {s:05.2f} secs\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# -------- Dataset Class -----------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# ------- Model Definition ----------\n",
    "class MurilClassifier(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=DRPT):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        if not BERT_TRAINABLE:\n",
    "            for p in self.bert.parameters(): p.requires_grad = False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.ELU()\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # he_uniform init\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.out.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = out.pooler_output if out.pooler_output is not None else out.last_hidden_state[:,0]\n",
    "        x = self.dropout(pooled)\n",
    "        x = self.act(self.fc1(x))\n",
    "        return self.sig(self.out(x)).squeeze(-1)\n",
    "\n",
    "# ----- Threshold Optimization ------\n",
    "def optimize_threshold(y_true, y_probs):\n",
    "    best, best_thresh = 0, .5\n",
    "    for t in np.arange(0.1, 0.9, 0.001):\n",
    "        f = f1_score(y_true, (y_probs>=t).astype(int))\n",
    "        if f>best: best, best_thresh = f, t\n",
    "    return best_thresh\n",
    "\n",
    "# ---------- Data Loading -----------\n",
    "########################################################\n",
    "def WriteResutls(reports):\n",
    "  unt0 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  tin1 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  macroavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  weightedavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  accu = []\n",
    "  for report in reports:\n",
    "    for k,v in report.items():\n",
    "      if 'UNT' in k:\n",
    "        unt0['precision'].append(v['precision'])\n",
    "        unt0['recall'].append(v['recall'])\n",
    "        unt0['f1-score'].append(v['f1-score'])\n",
    "      elif 'TIN' in k:\n",
    "        tin1['precision'].append(v['precision'])\n",
    "        tin1['recall'].append(v['recall'])\n",
    "        tin1['f1-score'].append(v['f1-score'])\n",
    "      elif 'macro avg' in k:\n",
    "        macroavg['precision'].append(v['precision'])\n",
    "        macroavg['recall'].append(v['recall'])\n",
    "        macroavg['f1-score'].append(v['f1-score'])\n",
    "      elif 'weighted avg' in k:\n",
    "        weightedavg['precision'].append(v['precision'])\n",
    "        weightedavg['recall'].append(v['recall'])\n",
    "        weightedavg['f1-score'].append(v['f1-score'])\n",
    "      else:\n",
    "        accu.append(v)\n",
    "\n",
    "  print('Accuracy:',np.mean(accu))\n",
    "  print(\"\")\n",
    "  print('UNT 0 Precision:',np.mean(unt0['precision']))\n",
    "  print('UNT 0 Recall:',np.mean(unt0['recall']))\n",
    "  print('UNT 0 F1-Score:',np.mean(unt0['f1-score']))\n",
    "  print(\"\")\n",
    "  print('TIN 1 Precision:',np.mean(tin1['precision']))\n",
    "  print('TIN 1 Recall:',np.mean(tin1['recall']))\n",
    "  print('TIN 1 F1-Score:',np.mean(tin1['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Weighted Avg Precision:',np.mean(weightedavg['precision']))\n",
    "  print('Weighted Avg Recall:',np.mean(weightedavg['recall']))\n",
    "  print('Weighted Avg F1-Score:',np.mean(weightedavg['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Macro  Precision:',np.mean(macroavg['precision']))\n",
    "  print('Macro  Recall:',np.mean(macroavg['recall']))\n",
    "  print('Macro  F1-Score:',np.mean(macroavg['f1-score']))\n",
    "\n",
    "\n",
    "  file = open( result_path, mode='a' )\n",
    "  file.write( modelname+ ' ( '+ datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +') \\n' )\n",
    "  file.write( 'Accuracy:'+str(np.mean(accu))+'\\n' )\n",
    "  file.write('UNT 0 Precision:'+str(np.mean(unt0['precision']))+'\\n' )\n",
    "  file.write('UNT 0 Recall:'+str(np.mean(unt0['recall']))+'\\n' )\n",
    "  file.write('UNT 0 F1-Score:'+str(np.mean(unt0['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('TIN 1 Precision:'+str(np.mean(tin1['precision']))+'\\n' )\n",
    "  file.write('TIN 1 Recall:'+str(np.mean(tin1['recall']))+'\\n' )\n",
    "  file.write('TIN 1 F1-Score:'+str(np.mean(tin1['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Weighted Avg Precision:'+str(np.mean(weightedavg['precision']))+'\\n' )\n",
    "  file.write('Weighted Avg Recall:'+str(np.mean(weightedavg['recall']))+'\\n' )\n",
    "  file.write('Weighted Avg F1-Score:'+str(np.mean(weightedavg['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Macro  Precision:'+str(np.mean(macroavg['precision']))+'\\n' )\n",
    "  file.write('Macro  Recall:'+str(np.mean(macroavg['recall']))+'\\n' )\n",
    "  file.write('Macro  F1-Score:'+str(np.mean(macroavg['f1-score']))+'\\n' )\n",
    "  file.close()\n",
    "  print(\"Done\")\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# -------------- Data Loading ---------------\n",
    "result_path =  r'C:\\Users\\mojua\\Desktop\\DL-Code\\T2-Classification-Result.csv'\n",
    "file_path = r'C:\\Users\\mojua\\Desktop\\DL-Code\\Dataset\\Offensive-24K-T2.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.columns, df.shape)\n",
    "gc.collect()\n",
    "\n",
    "xcolumn = 'Tweet'\n",
    "ycolumn = 'Tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 05:21 PM\n",
      "Fold1 Ep1: Val MacroF1=0.4763 Pat=0\n",
      "Fold1 Ep2: Val MacroF1=0.5724 Pat=0\n",
      "Fold1 Ep3: Val MacroF1=0.6186 Pat=0\n",
      "Fold1 Ep4: Val MacroF1=0.7183 Pat=0\n",
      "Fold1 Ep5: Val MacroF1=0.7283 Pat=0\n",
      "Fold1 Ep6: Val MacroF1=0.6606 Pat=1\n",
      "Fold1 Ep7: Val MacroF1=0.7242 Pat=2\n",
      "Fold1 Ep8: Val MacroF1=0.7242 Pat=3\n",
      "Fold1 Ep9: Val MacroF1=0.7222 Pat=4\n",
      "Fold1 done at 05:27 PM\n",
      "Fold2 Ep1: Val MacroF1=0.4299 Pat=0\n",
      "Fold2 Ep2: Val MacroF1=0.5779 Pat=0\n",
      "Fold2 Ep3: Val MacroF1=0.6347 Pat=0\n",
      "Fold2 Ep4: Val MacroF1=0.7079 Pat=0\n",
      "Fold2 Ep5: Val MacroF1=0.7546 Pat=0\n",
      "Fold2 Ep6: Val MacroF1=0.7564 Pat=0\n",
      "Fold2 Ep7: Val MacroF1=0.7622 Pat=0\n",
      "Fold2 Ep8: Val MacroF1=0.7708 Pat=0\n",
      "Fold2 Ep9: Val MacroF1=0.7692 Pat=1\n",
      "Fold2 Ep10: Val MacroF1=0.7675 Pat=2\n",
      "Fold2 Ep11: Val MacroF1=0.7691 Pat=3\n",
      "Fold2 Ep12: Val MacroF1=0.7655 Pat=4\n",
      "Fold2 done at 05:34 PM\n",
      "Fold3 Ep1: Val MacroF1=0.4951 Pat=0\n",
      "Fold3 Ep2: Val MacroF1=0.6631 Pat=0\n",
      "Fold3 Ep3: Val MacroF1=0.7046 Pat=0\n",
      "Fold3 Ep4: Val MacroF1=0.7212 Pat=0\n",
      "Fold3 Ep5: Val MacroF1=0.7169 Pat=1\n",
      "Fold3 Ep6: Val MacroF1=0.7610 Pat=0\n",
      "Fold3 Ep7: Val MacroF1=0.7455 Pat=1\n",
      "Fold3 Ep8: Val MacroF1=0.7383 Pat=2\n",
      "Fold3 Ep9: Val MacroF1=0.7324 Pat=3\n",
      "Fold3 Ep10: Val MacroF1=0.7464 Pat=4\n",
      "Fold3 done at 05:40 PM\n",
      "Fold4 Ep1: Val MacroF1=0.4335 Pat=0\n",
      "Fold4 Ep2: Val MacroF1=0.5658 Pat=0\n",
      "Fold4 Ep3: Val MacroF1=0.6270 Pat=0\n",
      "Fold4 Ep4: Val MacroF1=0.6645 Pat=0\n",
      "Fold4 Ep5: Val MacroF1=0.6894 Pat=0\n",
      "Fold4 Ep6: Val MacroF1=0.7564 Pat=0\n",
      "Fold4 Ep7: Val MacroF1=0.7203 Pat=1\n",
      "Fold4 Ep8: Val MacroF1=0.7003 Pat=2\n",
      "Fold4 Ep9: Val MacroF1=0.7556 Pat=3\n",
      "Fold4 Ep10: Val MacroF1=0.7110 Pat=4\n",
      "Fold4 done at 05:46 PM\n",
      "Fold5 Ep1: Val MacroF1=0.4631 Pat=0\n",
      "Fold5 Ep2: Val MacroF1=0.6153 Pat=0\n",
      "Fold5 Ep3: Val MacroF1=0.6015 Pat=1\n",
      "Fold5 Ep4: Val MacroF1=0.6839 Pat=0\n",
      "Fold5 Ep5: Val MacroF1=0.6744 Pat=1\n",
      "Fold5 Ep6: Val MacroF1=0.7110 Pat=0\n",
      "Fold5 Ep7: Val MacroF1=0.6956 Pat=1\n",
      "Fold5 Ep8: Val MacroF1=0.6956 Pat=2\n",
      "Fold5 Ep9: Val MacroF1=0.6956 Pat=3\n",
      "Fold5 Ep10: Val MacroF1=0.6908 Pat=4\n",
      "Fold5 done at 05:52 PM\n",
      "Accuracy: 0.8267852812091722\n",
      "\n",
      "UNT 0 Precision: 0.6998316899622802\n",
      "UNT 0 Recall: 0.5254041570438799\n",
      "UNT 0 F1-Score: 0.599624982015097\n",
      "\n",
      "TIN 1 Precision: 0.8560420649385378\n",
      "TIN 1 Recall: 0.9256897843011156\n",
      "TIN 1 F1-Score: 0.8894590895373293\n",
      "\n",
      "Weighted Avg Precision: 0.8174445133362618\n",
      "Weighted Avg Recall: 0.8267852812091722\n",
      "Weighted Avg F1-Score: 0.8178453706697413\n",
      "\n",
      "Macro  Precision: 0.777936877450409\n",
      "Macro  Recall: 0.7255469706724977\n",
      "Macro  F1-Score: 0.7445420357762131\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ----- K-Fold Training -------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# ---------- Tokenizer -------------\n",
    "bertmodelname = 'google/muril-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bertmodelname)\n",
    "# metrics storage\n",
    "val_acc, val_prec, val_rec, val_f1, val_macro = [], [], [], [], []\n",
    "test_acc, test_prec, test_rec, test_f1, test_macro = [], [], [], [], []\n",
    "reports = []\n",
    "\n",
    "start = time.time()\n",
    "print('Start:', time.strftime('%I:%M %p'))\n",
    "\n",
    "for fold,(tr,te) in enumerate(skf.split(df[xcolumn], df[ycolumn]),1):\n",
    "    X_tr, X_te = df.loc[tr,xcolumn].tolist(), df.loc[te,xcolumn].tolist()\n",
    "    y_tr, y_te = df.loc[tr,ycolumn].values, df.loc[te,ycolumn].values\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_tr,y_tr,test_size=0.15,random_state=0)\n",
    "    # tokenize\n",
    "    enc_tr = tokenizer(X_tr,padding='max_length',truncation=True,max_length=MAX_LEN)\n",
    "    enc_val = tokenizer(X_val,padding='max_length',truncation=True,max_length=MAX_LEN)\n",
    "    enc_te = tokenizer(X_te,padding='max_length',truncation=True,max_length=MAX_LEN)\n",
    "    # datasets/loaders\n",
    "    ds_tr = TextDataset(enc_tr,y_tr); \n",
    "    ds_val=TextDataset(enc_val,y_val); \n",
    "    ds_te=TextDataset(enc_te,y_te)\n",
    "    ld_tr = DataLoader(ds_tr,batch_size=BATCH,shuffle=True);\n",
    "    ld_val=DataLoader(ds_val,batch_size=BATCH); \n",
    "    ld_te=DataLoader(ds_te,batch_size=BATCH)\n",
    "    # model, optim, loss\n",
    "    model = MurilClassifier(bertmodelname).to(device)\n",
    "    crit = nn.BCELoss(); \n",
    "    opt = torch.optim.Adam(model.parameters(),lr=LR_RATE)\n",
    "    best_val,pat=0,0\n",
    "    # train\n",
    "    for ep in range(NEPOCHS):\n",
    "        model.train()\n",
    "        for b in ld_tr:\n",
    "            opt.zero_grad()\n",
    "            ids,mask,labels = b['input_ids'].to(device),b['attention_mask'].to(device),b['labels'].to(device)\n",
    "            probs = model(ids,mask)\n",
    "            loss = crit(probs,labels)\n",
    "            loss.backward(); opt.step()\n",
    "        # validate\n",
    "        model.eval(); \n",
    "        vp,vt=[],[]\n",
    "        with torch.no_grad():\n",
    "            for b in ld_val:\n",
    "                ids,mask = b['input_ids'].to(device),b['attention_mask'].to(device)\n",
    "                vp.extend(model(ids,mask).cpu().numpy()); \n",
    "                vt.extend(b['labels'].numpy())\n",
    "        thr=optimize_threshold(np.array(vt),np.array(vp))\n",
    "        preds=(np.array(vp)>=thr).astype(int)\n",
    "        # overall metrics\n",
    "        va=accuracy_score(vt,preds); \n",
    "        vp_=precision_score(vt,preds)\n",
    "        vr=recall_score(vt,preds); \n",
    "        vf=f1_score(vt,preds)\n",
    "        vm=f1_score(vt,preds,average='macro')\n",
    "        # per-class\n",
    "        vpc=precision_score(vt,preds,average=None); \n",
    "        vrc=recall_score(vt,preds,average=None)\n",
    "        vfc=f1_score(vt,preds,average=None)\n",
    "        # update best\n",
    "        if vm>best_val:\n",
    "            best_val=vm; \n",
    "            pat=0\n",
    "            torch.save(model.state_dict(),os.path.join(modelpath,f\"{modelname}_fold{fold}.pt\"))\n",
    "        else:\n",
    "            pat+=1\n",
    "        if DECAY and pat%DECAY_AFTER==0 and pat!=0:\n",
    "            for g in opt.param_groups: g['lr']*=DECAY_RATE\n",
    "        print(f\"Fold{fold} Ep{ep+1}: Val MacroF1={vm:.4f} Pat={pat}\")\n",
    "        if pat>=PATIENCE: break\n",
    "    # load best\n",
    "    model.load_state_dict(torch.load(os.path.join(modelpath,f\"{modelname}_fold{fold}.pt\")))\n",
    "    model.eval(); \n",
    "    vp,vt, tp,tt=[],[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for b in ld_val:\n",
    "            out=model(b['input_ids'].to(device),b['attention_mask'].to(device))\n",
    "            vp.extend(out.cpu().numpy());\n",
    "            vt.extend(b['labels'].numpy())\n",
    "        for b in ld_te:\n",
    "            out=model(b['input_ids'].to(device),b['attention_mask'].to(device))\n",
    "            tp.extend(out.cpu().numpy()); \n",
    "            tt.extend(b['labels'].numpy())\n",
    "    # threshold and metrics\n",
    "    thr=optimize_threshold(np.array(vt),np.array(vp))\n",
    "    vpred=(np.array(vp)>=thr).astype(int); \n",
    "    tpred=(np.array(tp)>=thr).astype(int)\n",
    "    reports.append(classification_report( tt, tpred, output_dict=True, zero_division=0, target_names=['UNT 0', 'TIN 1']))  \n",
    "    print(f\"Fold{fold} done at {time.strftime('%I:%M %p')}\")\n",
    "    # cleanup\n",
    "    del model; torch.cuda.empty_cache(); gc.collect()\n",
    "    \n",
    "WriteResutls(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Tweet  Tag\n",
      "0           0  USER دو بے نسلئیے ، حرامخور منافق مل رہے ہیں پ...    0\n",
      "1           3                   USER گھٹیا انسان دنیا ہی چھوڑ دو    1\n",
      "2          11  USER PMLN میں آپ کے بارے میں میری بہتر راۓ تھی...    1\n",
      "3          15  USER اسپین کی ٹیم بھی پاکستان کی کرکٹ ٹیم کی ط...    0\n",
      "4          20  ہمیں تو آج تک سمجھ نہیں آئی کہ کم عقل عیسائی ح...    1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8758 entries, 0 to 8757\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8758 non-null   int64 \n",
      " 1   Tweet       8758 non-null   object\n",
      " 2   Tag         8758 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "None\n",
      "Index(['Unnamed: 0', 'Tweet', 'Tag'], dtype='object') (8758, 3)\n"
     ]
    }
   ],
   "source": [
    "# -------------- Data Loading ---------------\n",
    "import gc, os, time, numpy as np, pandas as pd, datetime\n",
    "file_path = r'C:\\Users\\mojua\\Desktop\\DL-Code\\Dataset\\Offensive-24K-T2.xlsx'\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.columns, df.shape)\n",
    "gc.collect()\n",
    "\n",
    "xcolumn = 'Tweet'\n",
    "ycolumn = 'Tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um8bhzeKzC37",
    "outputId": "44e06191-3886-4dec-8eda-afb83ffa7fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc, os, time, numpy as np, pandas as pd, datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaConfig, XLMRobertaModel, logging\n",
    "\n",
    "# ---------------- GPU Setup ----------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "# ------------- Hyperparameters ------------\n",
    "MAX_LEN = 128\n",
    "EMBED_SIZE = 768  # same as hidden_size\n",
    "BERT_TRAINABLE = True\n",
    "DRPT = 0.4\n",
    "FC_WEIGHTS_INIT = 'he_uniform'\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 9e-6\n",
    "BATCH = 32\n",
    "NEPOCHS = 20\n",
    "PATIENCE = 4\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1\n",
    "result_path =  r'C:\\Users\\mojua\\Desktop\\DL-Code\\T2-Classification-Result.csv'\n",
    "modelname = 'hfFineTuneRoberta'\n",
    "modelpath = os.path.join('.', 'Saved Models', modelname)\n",
    "modelresults = os.path.join('.', 'Model Results')\n",
    "modelsummaries = os.path.join('.', 'Model - Summaries-Figures')\n",
    "for d in [modelpath, modelresults, modelsummaries]: os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# --------------- Utils ---------------------\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / 3600)\n",
    "    m = int((sec_elapsed % 3600) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return f\"{h} hrs {m:02d} mins {s:05.2f} secs\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# ----------- Dataset Class ----------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# --------- Model Definition ---------------\n",
    "class RobertaClassifier(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=DRPT):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "        self.bert = XLMRobertaModel.from_pretrained(model_name)\n",
    "        if not BERT_TRAINABLE:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.ELU()\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # weight initialization matching 'he_uniform'\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.out.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output if outputs.pooler_output is not None else outputs.last_hidden_state[:, 0]\n",
    "        x = self.dropout(pooled)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.sigmoid(self.out(x).squeeze(-1))\n",
    "        return x\n",
    "\n",
    "# --------- Threshold Optimization ----------\n",
    "def optimize_threshold(y_true, y_pred_probs):\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    for thresh in np.arange(0.1, 0.9, 0.001):\n",
    "        preds = (y_pred_probs >= thresh).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "    return best_thresh\n",
    "\n",
    "# -------- K-Fold Training -----------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "ba43741fafda4a9b83023e784aa66e2d",
      "3870f9ff6f074f2c8aa4aefa9306a307",
      "1f6b9263d7584fc8896150e000b709c4",
      "3855a0b5bca044b19f100401742dd964",
      "64ffb2d3b5094507b087138dc56bac86",
      "e2faa613e35e4887975945574617ce02",
      "2ddf2f0fb85a4f46859bc50f48d382ca",
      "638805ab4f1b4f3da186465fbd36bf01",
      "afbd6222b9b64c79a8ed95cf1ae954d4",
      "cd20a7cda9d54cbc9cc87b2a7c29d16f",
      "2b28c9309beb47e5981a56d670207250",
      "08ac0e9b1e00432d91c6749339937a43",
      "4234ba8d5ff846dc8b9a0dac23e5c3b4",
      "7ce13a71ee7a4289b03724abdb5be89d",
      "cb04ab03187e4e06a42a7366bc953f8e",
      "4169f5e47e464366841151514c72f7a3",
      "45c1b030b875474cabe57f162a65e175",
      "7ef02693da264329aa36de97b45bb154",
      "d9314836e1634b53a3c9f5b9d5c23329",
      "4bb8a29050c24e73b3fb26eb31bcc80c",
      "edbfb62bb6284c7cb0da1aebd851e04c",
      "4eb0760d72924bef927cbc661bccbd5b",
      "a81faf0a4a6a4834b9b91363a2ad6ba6",
      "d69b704def054170a3f2ad44e3ef37ab",
      "2fe226f85c764d45a5bc63044f156763",
      "0999a9d5c20049a8b3ffe426c35bcf3b",
      "bfd633b618ea4efb8a457d1f80cecd65",
      "d26fcb3e4551450abe815a8ad757e91b",
      "9704dc4aaa374c34982d9545fe9760ea",
      "7cc8e85ba79347d58a2c417f8d3c72a8",
      "07cd1e6af8da4e519647110727ae65cd",
      "ccbe4e6af97e45ac8019e9b958771187",
      "a1120e87b34446109cd377964e2cfa45",
      "0626dee929b245f3a3060f36e04e56bf",
      "f9cb13080f3e412f9993d3624beae077",
      "6e1d2517eb044bcebd077ca6408f258a",
      "55e64cf8425f46e783f9103899ee1cd1",
      "055bc683a2e74bdeb392dd1957ccb1b6",
      "a3a8ae066d7f455f8a4fc481e78d89e2",
      "a693dc69f27f481db7efd455c1ebbeeb",
      "67b7c958c2f4452b8ed0d1bdfa31b1f0",
      "ffed429885e4429f95301cbdd6da0726",
      "aeb3673b112e4620b8c712ab7979a97d",
      "cb61fad0e7c6435997550ecc29a17e6c"
     ]
    },
    "id": "oJFLunTZ1Hfq",
    "outputId": "051cb942-6ddd-4cf6-bb31-83a92c8b6a4b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Tokenizer Setup --------------\n",
    "bertmodelname = 'xlm-roberta-base'\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(bertmodelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rdee_Ajk5uEZ",
    "outputId": "d8997708-7fc6-45b9-a92b-a4d4b5623e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile length: 75\n",
      "95th percentile length: 81\n",
      "95th percentile length: 93\n"
     ]
    }
   ],
   "source": [
    "texts = df['Tweet'].tolist()\n",
    "\n",
    "# Get token lengths\n",
    "lengths = [len(tokenizer.encode(text, truncation=False)) for text in texts]\n",
    "\n",
    "# Percentiles\n",
    "p90 = int(np.percentile(lengths, 90))\n",
    "p95 = int(np.percentile(lengths, 95))\n",
    "p99 = int(np.percentile(lengths, 99))\n",
    "max_len = p95  # or use p90 for stricter cutoff\n",
    "\n",
    "print(f\"90th percentile length: {p90}\")\n",
    "print(f\"95th percentile length: {p95}\")\n",
    "print(f\"95th percentile length: {p99}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "def WriteResutls(reports):\n",
    "\n",
    "  unt0 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  tin1 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  macroavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  weightedavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  accu = []\n",
    "  for report in reports:\n",
    "    for k,v in report.items():\n",
    "      if 'UNT' in k:\n",
    "        unt0['precision'].append(v['precision'])\n",
    "        unt0['recall'].append(v['recall'])\n",
    "        unt0['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'TIN' in k:\n",
    "        tin1['precision'].append(v['precision'])\n",
    "        tin1['recall'].append(v['recall'])\n",
    "        tin1['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'macro avg' in k:\n",
    "        macroavg['precision'].append(v['precision'])\n",
    "        macroavg['recall'].append(v['recall'])\n",
    "        macroavg['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'weighted avg' in k:\n",
    "        weightedavg['precision'].append(v['precision'])\n",
    "        weightedavg['recall'].append(v['recall'])\n",
    "        weightedavg['f1-score'].append(v['f1-score'])\n",
    "      else:\n",
    "        accu.append(v)\n",
    "\n",
    "  print('Accuracy:',np.mean(accu))\n",
    "  print(\"\")\n",
    "  print('UNT 0 Precision:',np.mean(unt0['precision']))\n",
    "  print('UNT 0 Recall:',np.mean(unt0['recall']))\n",
    "  print('UNT 0 F1-Score:',np.mean(unt0['f1-score']))\n",
    "  print(\"\")\n",
    "  print('TIN 1 Precision:',np.mean(tin1['precision']))\n",
    "  print('TIN 1 Recall:',np.mean(tin1['recall']))\n",
    "  print('TIN 1 F1-Score:',np.mean(tin1['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Weighted Avg Precision:',np.mean(weightedavg['precision']))\n",
    "  print('Weighted Avg Recall:',np.mean(weightedavg['recall']))\n",
    "  print('Weighted Avg F1-Score:',np.mean(weightedavg['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Macro  Precision:',np.mean(macroavg['precision']))\n",
    "  print('Macro  Recall:',np.mean(macroavg['recall']))\n",
    "  print('Macro  F1-Score:',np.mean(macroavg['f1-score']))\n",
    "\n",
    "\n",
    "  file = open( result_path, mode='a' )\n",
    "  file.write( modelname+ ' ( '+ datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +') \\n' )\n",
    "  file.write( 'Accuracy:'+str(np.mean(accu))+'\\n' )\n",
    "  file.write('UNT 0 Precision:'+str(np.mean(unt0['precision']))+'\\n' )\n",
    "  file.write('UNT 0 Recall:'+str(np.mean(unt0['recall']))+'\\n' )\n",
    "  file.write('UNT 0 F1-Score:'+str(np.mean(unt0['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('TIN 1 Precision:'+str(np.mean(tin1['precision']))+'\\n' )\n",
    "  file.write('TIN 1 Recall:'+str(np.mean(tin1['recall']))+'\\n' )\n",
    "  file.write('TIN 1 F1-Score:'+str(np.mean(tin1['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Weighted Avg Precision:'+str(np.mean(weightedavg['precision']))+'\\n' )\n",
    "  file.write('Weighted Avg Recall:'+str(np.mean(weightedavg['recall']))+'\\n' )\n",
    "  file.write('Weighted Avg F1-Score:'+str(np.mean(weightedavg['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Macro  Precision:'+str(np.mean(macroavg['precision']))+'\\n' )\n",
    "  file.write('Macro  Recall:'+str(np.mean(macroavg['recall']))+'\\n' )\n",
    "  file.write('Macro  F1-Score:'+str(np.mean(macroavg['f1-score']))+'\\n' )\n",
    "  file.close()\n",
    "  print(\"Done\")\n",
    "\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "74c5a71c6d4047dd9c66e59ce4173750",
      "a31d867fb8e04d09997269a4a100f8f3",
      "cafcff3b21c446369358c23a5d388aa9",
      "d0a6dd425f104139a415302c31bc8d70",
      "24197ce2f45c41b5ae194469d2f9cd4c",
      "8218a57782d7495394945cfbbf87213e",
      "b5226f27887a4616bf2848a342812083",
      "fd26b947dba3415c8245a053cef46110",
      "263d73ba90a14edf8a4ba379627e9b2d",
      "e43ab3e305c140e2afe7e27c31b5d860",
      "1151e5a047f041f9ac4c8b830d9061f2"
     ]
    },
    "id": "thpLpPBx1Io9",
    "outputId": "d74332a8-eb74-49ca-f346-f1bb73b47778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local System Time: 08:12 PM\n",
      "Fold 1 Epoch 1/20 - Val Macro F1: 0.4806 - Patience: 0\n",
      "Fold 1 Epoch 2/20 - Val Macro F1: 0.6064 - Patience: 0\n",
      "Fold 1 Epoch 3/20 - Val Macro F1: 0.6777 - Patience: 0\n",
      "Fold 1 Epoch 4/20 - Val Macro F1: 0.6258 - Patience: 1\n",
      "Fold 1 Epoch 5/20 - Val Macro F1: 0.7283 - Patience: 0\n",
      "Fold 1 Epoch 6/20 - Val Macro F1: 0.7363 - Patience: 0\n",
      "Fold 1 Epoch 7/20 - Val Macro F1: 0.7201 - Patience: 1\n",
      "Fold 1 Epoch 8/20 - Val Macro F1: 0.7169 - Patience: 2\n",
      "Fold 1 Epoch 9/20 - Val Macro F1: 0.7283 - Patience: 3\n",
      "Fold 1 Epoch 10/20 - Val Macro F1: 0.7200 - Patience: 4\n",
      "Stopping early at epoch 10\n",
      "Completed fold 1/5\n",
      "Fold 2 Epoch 1/20 - Val Macro F1: 0.5306 - Patience: 0\n",
      "Fold 2 Epoch 2/20 - Val Macro F1: 0.6772 - Patience: 0\n",
      "Fold 2 Epoch 3/20 - Val Macro F1: 0.7372 - Patience: 0\n",
      "Fold 2 Epoch 4/20 - Val Macro F1: 0.7844 - Patience: 0\n",
      "Fold 2 Epoch 5/20 - Val Macro F1: 0.8068 - Patience: 0\n",
      "Fold 2 Epoch 6/20 - Val Macro F1: 0.7917 - Patience: 1\n",
      "Fold 2 Epoch 7/20 - Val Macro F1: 0.7822 - Patience: 2\n",
      "Fold 2 Epoch 8/20 - Val Macro F1: 0.7826 - Patience: 3\n",
      "Fold 2 Epoch 9/20 - Val Macro F1: 0.7801 - Patience: 4\n",
      "Stopping early at epoch 9\n",
      "Completed fold 2/5\n",
      "Fold 3 Epoch 1/20 - Val Macro F1: 0.4869 - Patience: 0\n",
      "Fold 3 Epoch 2/20 - Val Macro F1: 0.6359 - Patience: 0\n",
      "Fold 3 Epoch 3/20 - Val Macro F1: 0.6832 - Patience: 0\n",
      "Fold 3 Epoch 4/20 - Val Macro F1: 0.7492 - Patience: 0\n",
      "Fold 3 Epoch 5/20 - Val Macro F1: 0.7684 - Patience: 0\n",
      "Fold 3 Epoch 6/20 - Val Macro F1: 0.7423 - Patience: 1\n",
      "Fold 3 Epoch 7/20 - Val Macro F1: 0.7405 - Patience: 2\n",
      "Fold 3 Epoch 8/20 - Val Macro F1: 0.7501 - Patience: 3\n",
      "Fold 3 Epoch 9/20 - Val Macro F1: 0.7511 - Patience: 4\n",
      "Stopping early at epoch 9\n",
      "Completed fold 3/5\n",
      "Fold 4 Epoch 1/20 - Val Macro F1: 0.4496 - Patience: 0\n",
      "Fold 4 Epoch 2/20 - Val Macro F1: 0.6165 - Patience: 0\n",
      "Fold 4 Epoch 3/20 - Val Macro F1: 0.7157 - Patience: 0\n",
      "Fold 4 Epoch 4/20 - Val Macro F1: 0.7356 - Patience: 0\n",
      "Fold 4 Epoch 5/20 - Val Macro F1: 0.7705 - Patience: 0\n",
      "Fold 4 Epoch 6/20 - Val Macro F1: 0.7802 - Patience: 0\n",
      "Fold 4 Epoch 7/20 - Val Macro F1: 0.7742 - Patience: 1\n",
      "Fold 4 Epoch 8/20 - Val Macro F1: 0.7768 - Patience: 2\n",
      "Fold 4 Epoch 9/20 - Val Macro F1: 0.7667 - Patience: 3\n",
      "Fold 4 Epoch 10/20 - Val Macro F1: 0.7652 - Patience: 4\n",
      "Stopping early at epoch 10\n",
      "Completed fold 4/5\n",
      "Fold 5 Epoch 1/20 - Val Macro F1: 0.6123 - Patience: 0\n",
      "Fold 5 Epoch 2/20 - Val Macro F1: 0.6156 - Patience: 0\n",
      "Fold 5 Epoch 3/20 - Val Macro F1: 0.7208 - Patience: 0\n",
      "Fold 5 Epoch 4/20 - Val Macro F1: 0.7607 - Patience: 0\n",
      "Fold 5 Epoch 5/20 - Val Macro F1: 0.7745 - Patience: 0\n",
      "Fold 5 Epoch 6/20 - Val Macro F1: 0.7492 - Patience: 1\n",
      "Fold 5 Epoch 7/20 - Val Macro F1: 0.7543 - Patience: 2\n",
      "Fold 5 Epoch 8/20 - Val Macro F1: 0.7701 - Patience: 3\n",
      "Fold 5 Epoch 9/20 - Val Macro F1: 0.7279 - Patience: 4\n",
      "Stopping early at epoch 9\n",
      "Completed fold 5/5\n",
      "Total runtime: 0 hrs 50 mins 33.34 secs\n",
      "Accuracy: 0.8259871071716358\n",
      "\n",
      "UNT 0 Precision: 0.8185353283381517\n",
      "UNT 0 Recall: 0.38717924044136515\n",
      "UNT 0 F1-Score: 0.5134238494342276\n",
      "\n",
      "TIN 1 Precision: 0.8293674035683347\n",
      "TIN 1 Recall: 0.9699695474453562\n",
      "TIN 1 F1-Score: 0.8937303491445627\n",
      "\n",
      "Weighted Avg Precision: 0.8266860065196786\n",
      "Weighted Avg Recall: 0.8259871071716358\n",
      "Weighted Avg F1-Score: 0.7997749842176404\n",
      "\n",
      "Macro  Precision: 0.8239513659532433\n",
      "Macro  Recall: 0.6785743939433606\n",
      "Macro  F1-Score: 0.7035770992893952\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "# Metric storage\n",
    "valaccuracy, valprecision, valrecall, valf1, valcm = [], [], [], [], []\n",
    "testaccuracy, testprecision, testrecall, testf1, testcm = [], [], [], [], []\n",
    "test_f1_macro = []\n",
    "com_text, com_label, com_predicted, com_prob = [], [], [], []\n",
    "com_indices = []\n",
    "reports = []\n",
    "start_time = time.time()\n",
    "print('Local System Time:', time.strftime('%I:%M %p', time.localtime()))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(df[xcolumn], df[ycolumn]), 1):\n",
    "    x_train = df.loc[train_idx, xcolumn].tolist()\n",
    "    y_train = df.loc[train_idx, ycolumn].values\n",
    "    x_test = df.loc[test_idx, xcolumn].tolist()\n",
    "    y_test = df.loc[test_idx, ycolumn].values\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=0)\n",
    "\n",
    "    train_enc = tokenizer(x_train, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    val_enc   = tokenizer(x_val, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    test_enc  = tokenizer(x_test, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    train_dataset = TextDataset(train_enc, y_train)\n",
    "    val_dataset   = TextDataset(val_enc, y_val)\n",
    "    test_dataset  = TextDataset(test_enc, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=BATCH)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=BATCH)\n",
    "\n",
    "    model = RobertaClassifier(bertmodelname).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "\n",
    "    best_val_f1 = -np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NEPOCHS):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            probs = model(input_ids, attention_mask)\n",
    "            loss = criterion(probs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_probs, val_trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                probs = model(input_ids, attention_mask).cpu().numpy()\n",
    "                val_probs.extend(probs.tolist())\n",
    "                val_trues.extend(labels.tolist())\n",
    "\n",
    "        thresh = optimize_threshold(np.array(val_trues), np.array(val_probs))\n",
    "        val_preds = (np.array(val_probs) >= thresh).astype(int)\n",
    "        # overall metrics\n",
    "        valaccuracy.append(accuracy_score(val_trues, val_preds))\n",
    "        valprecision.append(precision_score(val_trues, val_preds))\n",
    "        valrecall.append(recall_score(val_trues, val_preds))\n",
    "        valf1.append(f1_score(val_trues, val_preds))\n",
    "        valcm.append(confusion_matrix(val_trues, val_preds))\n",
    "\n",
    "        val_f1_macro_score = f1_score(val_trues, val_preds, average='macro')\n",
    "\n",
    "\n",
    "        # save best and early stop\n",
    "        if val_f1_macro_score > best_val_f1:\n",
    "            best_val_f1 = val_f1_macro_score\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), os.path.join(modelpath, f\"{modelname}_fold{fold}.bin\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if DECAY and patience_counter % DECAY_AFTER == 0 and patience_counter != 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] *= DECAY_RATE\n",
    "\n",
    "        print(f\"Fold {fold} Epoch {epoch+1}/{NEPOCHS} - Val Macro F1: {val_f1_macro_score:.4f} - Patience: {patience_counter}\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Evaluate on test\n",
    "    model.load_state_dict(torch.load(os.path.join(modelpath, f\"{modelname}_fold{fold}.bin\")))\n",
    "    model.eval()\n",
    "    test_probs, test_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            probs = logits.cpu().numpy()\n",
    "            test_probs.extend(probs.tolist())\n",
    "            test_trues.extend(batch['labels'].cpu().numpy().tolist())\n",
    "\n",
    "    test_preds = (np.array(test_probs) >= thresh).astype(int)\n",
    "    # overall test metrics\n",
    "    testaccuracy.append(accuracy_score(test_trues, test_preds))\n",
    "    testprecision.append(precision_score(test_trues, test_preds))\n",
    "    testrecall.append(recall_score(test_trues, test_preds))\n",
    "    testf1.append(f1_score(test_trues, test_preds))\n",
    "    testcm.append(confusion_matrix(test_trues, test_preds))\n",
    "    reports.append(classification_report( test_trues, test_preds, output_dict=True, zero_division=0, target_names=['UNT 0', 'TIN 1']))\n",
    "\n",
    "    com_indices.extend(test_idx.tolist())\n",
    "    com_text.extend(df.loc[test_idx, xcolumn].tolist())\n",
    "    com_label.extend(df.loc[test_idx, ycolumn].tolist())\n",
    "    com_predicted.extend(test_preds.tolist())\n",
    "    com_prob.extend(test_probs)\n",
    "\n",
    "    print(f\"Completed fold {fold}/{skf.get_n_splits()}\")\n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Total runtime: {hms_string(time.time() - start_time)}\")\n",
    "WriteResutls(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tRhHy24zC39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc, os,time, numpy as np,pandas as pd, datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import torch,torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, logging\n",
    "\n",
    "file_path = r'C:\\Users\\mojua\\Desktop\\DL-Code\\Dataset\\Offensive-24K-T2.xlsx'\n",
    "result_path =  r'C:\\Users\\mojua\\Desktop\\DL-Code\\T2-Classification-Result.csv'\n",
    "\n",
    "# ---------------- GPU Setup ----------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':torch.cuda.empty_cache()\n",
    "\n",
    "# --------- Hyperparameters ---------\n",
    "MAX_LEN = 130\n",
    "BERT_TRAINABLE = True\n",
    "DRPT = 0.4\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 9e-6\n",
    "BATCH = 32\n",
    "NEPOCHS = 20\n",
    "PATIENCE = 4\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1\n",
    "\n",
    "modelname = 'hfFineTuneDistilBert'\n",
    "modelpath = os.path.join('.', 'Saved Models', modelname)\n",
    "for d in [modelpath, './Model Results', './Model - Summaries-Figures']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "######################################################################################\n",
    "# -------- Utils --------\n",
    "def hms_string(sec):\n",
    "    h = int(sec // 3600)\n",
    "    m = int((sec % 3600) // 60)\n",
    "    s = sec % 60\n",
    "    return f\"{h} hrs {m:02d} mins {s:05.2f} secs\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# -------- Dataset --------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings, self.labels = encodings, labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# -------- Model --------\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=config)\n",
    "        if not BERT_TRAINABLE:\n",
    "            for p in self.bert.parameters(): p.requires_grad=False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(DRPT)\n",
    "        self.fc1 = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.ELU()\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # he_uniform initialization\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.out.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        o = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = o.last_hidden_state[:,0]\n",
    "        x = self.dropout(pooled)\n",
    "        x = self.act(self.fc1(x))\n",
    "        return self.sig(self.out(x).squeeze(-1))\n",
    "\n",
    "# -------- Threshold --------\n",
    "def optimize_threshold(y_true, y_probs):\n",
    "    best_t, best_f = 0.5, 0\n",
    "    for t in np.arange(0.1,0.9,0.001):\n",
    "        p = (y_probs>=t).astype(int)\n",
    "        f = f1_score(y_true,p)\n",
    "        if f>best_f:\n",
    "            best_f, best_t = f, t\n",
    "    return best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Tweet  Tag\n",
      "0           0  USER دو بے نسلئیے ، حرامخور منافق مل رہے ہیں پ...    0\n",
      "1           3                   USER گھٹیا انسان دنیا ہی چھوڑ دو    1\n",
      "2          11  USER PMLN میں آپ کے بارے میں میری بہتر راۓ تھی...    1\n",
      "3          15  USER اسپین کی ٹیم بھی پاکستان کی کرکٹ ٹیم کی ط...    0\n",
      "4          20  ہمیں تو آج تک سمجھ نہیں آئی کہ کم عقل عیسائی ح...    1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8758 entries, 0 to 8757\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8758 non-null   int64 \n",
      " 1   Tweet       8758 non-null   object\n",
      " 2   Tag         8758 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "None (8758, 3)\n"
     ]
    }
   ],
   "source": [
    "# -------- Data Loading --------\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "print(df.info(), df.shape)\n",
    "gc.collect()\n",
    "xcol, ycol = 'Tweet', 'Tag'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "def WriteResutls(reports):\n",
    "\n",
    "  unt0 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  tin1 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  macroavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  weightedavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  accu = []\n",
    "  for report in reports:\n",
    "    for k,v in report.items():\n",
    "      if 'UNT' in k:\n",
    "        unt0['precision'].append(v['precision'])\n",
    "        unt0['recall'].append(v['recall'])\n",
    "        unt0['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'TIN' in k:\n",
    "        tin1['precision'].append(v['precision'])\n",
    "        tin1['recall'].append(v['recall'])\n",
    "        tin1['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'macro avg' in k:\n",
    "        macroavg['precision'].append(v['precision'])\n",
    "        macroavg['recall'].append(v['recall'])\n",
    "        macroavg['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'weighted avg' in k:\n",
    "        weightedavg['precision'].append(v['precision'])\n",
    "        weightedavg['recall'].append(v['recall'])\n",
    "        weightedavg['f1-score'].append(v['f1-score'])\n",
    "      else:\n",
    "        accu.append(v)\n",
    "\n",
    "  print('Accuracy:',np.mean(accu))\n",
    "  print(\"\")\n",
    "  print('UNT 0 Precision:',np.mean(unt0['precision']))\n",
    "  print('UNT 0 Recall:',np.mean(unt0['recall']))\n",
    "  print('UNT 0 F1-Score:',np.mean(unt0['f1-score']))\n",
    "  print(\"\")\n",
    "  print('TIN 1 Precision:',np.mean(tin1['precision']))\n",
    "  print('TIN 1 Recall:',np.mean(tin1['recall']))\n",
    "  print('TIN 1 F1-Score:',np.mean(tin1['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Weighted Avg Precision:',np.mean(weightedavg['precision']))\n",
    "  print('Weighted Avg Recall:',np.mean(weightedavg['recall']))\n",
    "  print('Weighted Avg F1-Score:',np.mean(weightedavg['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Macro  Precision:',np.mean(macroavg['precision']))\n",
    "  print('Macro  Recall:',np.mean(macroavg['recall']))\n",
    "  print('Macro  F1-Score:',np.mean(macroavg['f1-score']))\n",
    "\n",
    "\n",
    "  file = open( result_path, mode='a' )\n",
    "  file.write( modelname+ ' ( '+ datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +') \\n' )\n",
    "  file.write( 'Accuracy:'+str(np.mean(accu))+'\\n' )\n",
    "  file.write('UNT 0 Precision:'+str(np.mean(unt0['precision']))+'\\n' )\n",
    "  file.write('UNT 0 Recall:'+str(np.mean(unt0['recall']))+'\\n' )\n",
    "  file.write('UNT 0 F1-Score:'+str(np.mean(unt0['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('TIN 1 Precision:'+str(np.mean(tin1['precision']))+'\\n' )\n",
    "  file.write('TIN 1 Recall:'+str(np.mean(tin1['recall']))+'\\n' )\n",
    "  file.write('TIN 1 F1-Score:'+str(np.mean(tin1['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Weighted Avg Precision:'+str(np.mean(weightedavg['precision']))+'\\n' )\n",
    "  file.write('Weighted Avg Recall:'+str(np.mean(weightedavg['recall']))+'\\n' )\n",
    "  file.write('Weighted Avg F1-Score:'+str(np.mean(weightedavg['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Macro  Precision:'+str(np.mean(macroavg['precision']))+'\\n' )\n",
    "  file.write('Macro  Recall:'+str(np.mean(macroavg['recall']))+'\\n' )\n",
    "  file.write('Macro  F1-Score:'+str(np.mean(macroavg['f1-score']))+'\\n' )\n",
    "  file.close()\n",
    "  print(\"Done\")\n",
    "\n",
    "######################################################################################\n",
    "# -------- Training Loop --------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# -------- Tokenizer & Config --------\n",
    "bertmodelname = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bertmodelname)\n",
    "config = AutoConfig.from_pretrained(bertmodelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1 Ep1/20 - ValMacroF1=0.5365 Pat=0\n",
      "Fold1 Ep2/20 - ValMacroF1=0.6447 Pat=0\n",
      "Fold1 Ep3/20 - ValMacroF1=0.6369 Pat=1\n",
      "Fold1 Ep4/20 - ValMacroF1=0.6351 Pat=2\n",
      "Fold1 Ep5/20 - ValMacroF1=0.6725 Pat=0\n",
      "Fold1 Ep6/20 - ValMacroF1=0.6709 Pat=1\n",
      "Fold1 Ep7/20 - ValMacroF1=0.6694 Pat=2\n",
      "Fold1 Ep8/20 - ValMacroF1=0.6704 Pat=3\n",
      "Fold1 Ep9/20 - ValMacroF1=0.6704 Pat=4\n",
      "Completed fold 1/5\n",
      "Fold2 Ep1/20 - ValMacroF1=0.5175 Pat=0\n",
      "Fold2 Ep2/20 - ValMacroF1=0.6782 Pat=0\n",
      "Fold2 Ep3/20 - ValMacroF1=0.6921 Pat=0\n",
      "Fold2 Ep4/20 - ValMacroF1=0.7135 Pat=0\n",
      "Fold2 Ep5/20 - ValMacroF1=0.7125 Pat=1\n",
      "Fold2 Ep6/20 - ValMacroF1=0.7113 Pat=2\n",
      "Fold2 Ep7/20 - ValMacroF1=0.7144 Pat=0\n",
      "Fold2 Ep8/20 - ValMacroF1=0.7251 Pat=0\n",
      "Fold2 Ep9/20 - ValMacroF1=0.7241 Pat=1\n",
      "Fold2 Ep10/20 - ValMacroF1=0.7174 Pat=2\n",
      "Fold2 Ep11/20 - ValMacroF1=0.7164 Pat=3\n",
      "Fold2 Ep12/20 - ValMacroF1=0.7164 Pat=4\n",
      "Completed fold 2/5\n",
      "Fold3 Ep1/20 - ValMacroF1=0.5830 Pat=0\n",
      "Fold3 Ep2/20 - ValMacroF1=0.7193 Pat=0\n",
      "Fold3 Ep3/20 - ValMacroF1=0.7529 Pat=0\n",
      "Fold3 Ep4/20 - ValMacroF1=0.7130 Pat=1\n",
      "Fold3 Ep5/20 - ValMacroF1=0.7593 Pat=0\n",
      "Fold3 Ep6/20 - ValMacroF1=0.7450 Pat=1\n",
      "Fold3 Ep7/20 - ValMacroF1=0.7397 Pat=2\n",
      "Fold3 Ep8/20 - ValMacroF1=0.7433 Pat=3\n",
      "Fold3 Ep9/20 - ValMacroF1=0.7442 Pat=4\n",
      "Completed fold 3/5\n",
      "Fold4 Ep1/20 - ValMacroF1=0.5807 Pat=0\n",
      "Fold4 Ep2/20 - ValMacroF1=0.6862 Pat=0\n",
      "Fold4 Ep3/20 - ValMacroF1=0.6691 Pat=1\n",
      "Fold4 Ep4/20 - ValMacroF1=0.6874 Pat=0\n",
      "Fold4 Ep5/20 - ValMacroF1=0.7178 Pat=0\n",
      "Fold4 Ep6/20 - ValMacroF1=0.6920 Pat=1\n",
      "Fold4 Ep7/20 - ValMacroF1=0.6911 Pat=2\n",
      "Fold4 Ep8/20 - ValMacroF1=0.6986 Pat=3\n",
      "Fold4 Ep9/20 - ValMacroF1=0.7079 Pat=4\n",
      "Completed fold 4/5\n",
      "Fold5 Ep1/20 - ValMacroF1=0.5454 Pat=0\n",
      "Fold5 Ep2/20 - ValMacroF1=0.6682 Pat=0\n",
      "Fold5 Ep3/20 - ValMacroF1=0.7157 Pat=0\n",
      "Fold5 Ep4/20 - ValMacroF1=0.7003 Pat=1\n",
      "Fold5 Ep5/20 - ValMacroF1=0.7043 Pat=2\n",
      "Fold5 Ep6/20 - ValMacroF1=0.7008 Pat=3\n",
      "Fold5 Ep7/20 - ValMacroF1=0.7008 Pat=4\n",
      "Completed fold 5/5\n",
      "Total runtime: 0 hrs 15 mins 14.55 secs\n",
      "Accuracy: 0.8024644756160211\n",
      "\n",
      "UNT 0 Precision: 0.7534677680545662\n",
      "UNT 0 Recall: 0.327077452741425\n",
      "UNT 0 F1-Score: 0.43460591171034224\n",
      "\n",
      "TIN 1 Precision: 0.813970554685632\n",
      "TIN 1 Recall: 0.9584480816731304\n",
      "TIN 1 F1-Score: 0.8796939152621652\n",
      "\n",
      "Weighted Avg Precision: 0.7990066979728532\n",
      "Weighted Avg Recall: 0.8024644756160211\n",
      "Weighted Avg F1-Score: 0.7697367462050384\n",
      "\n",
      "Macro  Precision: 0.783719161370099\n",
      "Macro  Recall: 0.6427627672072778\n",
      "Macro  F1-Score: 0.6571499134862537\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# storage\n",
    "valacc, valprec, valrec, valf1, valcm = [], [], [], [], []\n",
    "val_prec_pc, val_rec_pc, val_f1_pc, val_f1_macro = [], [], [], []\n",
    "testacc, testprec, testrec, testf1, testcm = [], [], [], [], []\n",
    "test_prec_pc, test_rec_pc, test_f1_pc, test_f1_macro = [], [], [], []\n",
    "reports =  []\n",
    "start = time.time()\n",
    "for fold, (tr, te) in enumerate(skf.split(df[xcol], df[ycol]), 1):\n",
    "    Xtr = df.loc[tr, xcol].tolist()\n",
    "    ytr = df.loc[tr, ycol].values\n",
    "    Xte = df.loc[te, xcol].tolist()\n",
    "    yte = df.loc[te, ycol].values\n",
    "\n",
    "    Xtr, Xv, ytr, yv = train_test_split(Xtr, ytr, test_size=0.15, random_state=0)\n",
    "    enc_tr = tokenizer(Xtr, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_v  = tokenizer(Xv,  padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_te = tokenizer(Xte, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    dt_tr = DataLoader(TextDataset(enc_tr, ytr), batch_size=BATCH, shuffle=True)\n",
    "    dt_v  = DataLoader(TextDataset(enc_v,  yv), batch_size=BATCH)\n",
    "    dt_te = DataLoader(TextDataset(enc_te, yte), batch_size=BATCH)\n",
    "\n",
    "    model = DistilBertClassifier(bertmodelname).to(device)\n",
    "    crit  = nn.BCELoss()\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "    best_f, pat = -np.inf, 0\n",
    "\n",
    "    for ep in range(NEPOCHS):\n",
    "        model.train()\n",
    "        for b in dt_tr:\n",
    "            opt.zero_grad()\n",
    "            ids   = b['input_ids'].to(device)\n",
    "            mask  = b['attention_mask'].to(device)\n",
    "            labels= b['labels'].to(device)\n",
    "            probs = model(ids, mask)\n",
    "            loss  = crit(probs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        # validation\n",
    "        model.eval()\n",
    "        vp, vt = [], []\n",
    "        with torch.no_grad():\n",
    "            for b in dt_v:\n",
    "                ids  = b['input_ids'].to(device)\n",
    "                mask = b['attention_mask'].to(device)\n",
    "                vp.extend(model(ids, mask).cpu().tolist())\n",
    "                vt.extend(b['labels'].cpu().tolist())\n",
    "        th    = optimize_threshold(np.array(vt), np.array(vp))\n",
    "        vpred = (np.array(vp) >= th).astype(int)\n",
    "        # record metrics\n",
    "        valacc.append(       accuracy_score(vt, vpred))\n",
    "        valprec.append(      precision_score(vt, vpred))\n",
    "        valrec.append(       recall_score(vt, vpred))\n",
    "        valf1.append(        f1_score(vt, vpred))\n",
    "        valcm.append(        confusion_matrix(vt, vpred))\n",
    "       \n",
    "        vf1m = f1_score(vt, vpred, average='macro')\n",
    "        val_f1_macro.append(vf1m)\n",
    "        # early stopping\n",
    "        if vf1m > best_f:\n",
    "            best_f, pat = vf1m, 0\n",
    "            torch.save(model.state_dict(),os.path.join(modelpath, f\"{modelname}_fold{fold}.bin\"))\n",
    "        else:\n",
    "            pat += 1\n",
    "        if DECAY and pat % DECAY_AFTER == 0 and pat != 0:\n",
    "            for g in opt.param_groups:\n",
    "                g['lr'] *= DECAY_RATE\n",
    "                \n",
    "        print(f\"Fold{fold} Ep{ep+1}/{NEPOCHS} - ValMacroF1={vf1m:.4f} Pat={pat}\")\n",
    "        if pat >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    # test evaluation\n",
    "    model.load_state_dict(torch.load(os.path.join(modelpath, f\"{modelname}_fold{fold}.bin\")))\n",
    "    model.eval()\n",
    "    tp, tt = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in dt_te:\n",
    "            ids  = b['input_ids'].to(device)\n",
    "            mask = b['attention_mask'].to(device)\n",
    "            tp.extend(model(ids, mask).cpu().tolist())\n",
    "            tt.extend(b['labels'].cpu().tolist())\n",
    "    tpred = (np.array(tp) >= th).astype(int)\n",
    "\n",
    "    testacc.append(       accuracy_score(tt, tpred))\n",
    "    testprec.append(      precision_score(tt, tpred))\n",
    "    testrec.append(       recall_score(tt, tpred))\n",
    "    testf1.append(        f1_score(tt, tpred))\n",
    "    testcm.append(        confusion_matrix(tt, tpred))\n",
    "    reports.append(classification_report( tt, tpred, output_dict=True, zero_division=0, target_names=['UNT 0', 'TIN 1']))\n",
    "\n",
    "    print(f\"Completed fold {fold}/5\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(\"Total runtime:\", hms_string(time.time() - start))\n",
    "WriteResutls(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from here after kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc, os,time, numpy as np,pandas as pd, datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import torch,torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, logging\n",
    "\n",
    "file_path = r'C:\\Users\\mojua\\Desktop\\DL-Code\\Dataset\\Offensive-24K-T2.xlsx'\n",
    "result_path =  r'C:\\Users\\mojua\\Desktop\\DL-Code\\T2-Classification-Result.csv'\n",
    "\n",
    "# ---------------- GPU Setup ----------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':torch.cuda.empty_cache()\n",
    "\n",
    "# --------- Hyperparameters ---------\n",
    "MAX_LEN = 128\n",
    "BERT_TRAINABLE = True\n",
    "DRPT = 0.4\n",
    "FC_ACT = 'elu'\n",
    "LR_RATE = 1e-5\n",
    "BATCH = 32\n",
    "NEPOCHS = 20\n",
    "PATIENCE = 4\n",
    "DECAY = True\n",
    "DECAY_RATE = 0.3\n",
    "DECAY_AFTER = 1\n",
    "modelname = 'hfFineTuneBert'\n",
    "bertmodelname = 'bert-base-multilingual-cased'\n",
    "\n",
    "modelpath = os.path.join('.', 'Saved Models', modelname)\n",
    "for d in [modelpath, './Model Results', './Model - Summaries-Figures']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "#################################################################################\n",
    "# -------- Utils --------\n",
    "def hms_string(sec):\n",
    "    h = int(sec // 3600)\n",
    "    m = int((sec % 3600) // 60)\n",
    "    s = sec % 60\n",
    "    return f\"{h} hrs {m:02d} mins {s:05.2f} secs\"\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "# ----------- Dataset Class ----------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "# -------- Model Definition ----------------\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=DRPT):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=config)\n",
    "        if not BERT_TRAINABLE:\n",
    "            for p in self.bert.parameters(): p.requires_grad = False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.ELU()\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # he_uniform init\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.out.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        o = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = o.pooler_output if hasattr(o, 'pooler_output') else o.last_hidden_state[:,0]\n",
    "        x = self.dropout(pooled)\n",
    "        x = self.act(self.fc1(x))\n",
    "        return self.sig(self.out(x).squeeze(-1))\n",
    "\n",
    "# -------- Threshold Optimization -----------\n",
    "def optimize_threshold(y_true, y_probs):\n",
    "    best_t, best_f = 0.5, 0\n",
    "    for t in np.arange(0.1,0.9,0.001):\n",
    "        p = (y_probs>=t).astype(int)\n",
    "        f = f1_score(y_true, p)\n",
    "        if f>best_f: best_f, best_t = f, t\n",
    "    return best_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Tweet  Tag\n",
      "0           0  USER دو بے نسلئیے ، حرامخور منافق مل رہے ہیں پ...    0\n",
      "1           3                   USER گھٹیا انسان دنیا ہی چھوڑ دو    1\n",
      "2          11  USER PMLN میں آپ کے بارے میں میری بہتر راۓ تھی...    1\n",
      "3          15  USER اسپین کی ٹیم بھی پاکستان کی کرکٹ ٹیم کی ط...    0\n",
      "4          20  ہمیں تو آج تک سمجھ نہیں آئی کہ کم عقل عیسائی ح...    1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8758 entries, 0 to 8757\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  8758 non-null   int64 \n",
      " 1   Tweet       8758 non-null   object\n",
      " 2   Tag         8758 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 205.4+ KB\n",
      "None\n",
      "Index(['Unnamed: 0', 'Tweet', 'Tag'], dtype='object') (8758, 3)\n"
     ]
    }
   ],
   "source": [
    "# -------------- Data Loading ---------------\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.columns, df.shape)\n",
    "gc.collect()\n",
    "xcolumn = 'Tweet'\n",
    "ycolumn = 'Tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "def WriteResutls(reports):\n",
    "\n",
    "  unt0 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  tin1 = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  macroavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  weightedavg = {'precision':[], 'recall':[], 'f1-score':[] }\n",
    "  accu = []\n",
    "  for report in reports:\n",
    "    for k,v in report.items():\n",
    "      if 'UNT' in k:\n",
    "        unt0['precision'].append(v['precision'])\n",
    "        unt0['recall'].append(v['recall'])\n",
    "        unt0['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'TIN' in k:\n",
    "        tin1['precision'].append(v['precision'])\n",
    "        tin1['recall'].append(v['recall'])\n",
    "        tin1['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'macro avg' in k:\n",
    "        macroavg['precision'].append(v['precision'])\n",
    "        macroavg['recall'].append(v['recall'])\n",
    "        macroavg['f1-score'].append(v['f1-score'])\n",
    "\n",
    "      elif 'weighted avg' in k:\n",
    "        weightedavg['precision'].append(v['precision'])\n",
    "        weightedavg['recall'].append(v['recall'])\n",
    "        weightedavg['f1-score'].append(v['f1-score'])\n",
    "      else:\n",
    "        accu.append(v)\n",
    "\n",
    "  print('Accuracy:',np.mean(accu))\n",
    "  print(\"\")\n",
    "  print('UNT 0 Precision:',np.mean(unt0['precision']))\n",
    "  print('UNT 0 Recall:',np.mean(unt0['recall']))\n",
    "  print('UNT 0 F1-Score:',np.mean(unt0['f1-score']))\n",
    "  print(\"\")\n",
    "  print('TIN 1 Precision:',np.mean(tin1['precision']))\n",
    "  print('TIN 1 Recall:',np.mean(tin1['recall']))\n",
    "  print('TIN 1 F1-Score:',np.mean(tin1['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Weighted Avg Precision:',np.mean(weightedavg['precision']))\n",
    "  print('Weighted Avg Recall:',np.mean(weightedavg['recall']))\n",
    "  print('Weighted Avg F1-Score:',np.mean(weightedavg['f1-score']))\n",
    "\n",
    "  print(\"\")\n",
    "  print('Macro  Precision:',np.mean(macroavg['precision']))\n",
    "  print('Macro  Recall:',np.mean(macroavg['recall']))\n",
    "  print('Macro  F1-Score:',np.mean(macroavg['f1-score']))\n",
    "\n",
    "\n",
    "  file = open( result_path, mode='a' )\n",
    "  file.write( modelname+ ' ( '+ datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +') \\n' )\n",
    "  file.write( 'Accuracy:'+str(np.mean(accu))+'\\n' )\n",
    "  file.write('UNT 0 Precision:'+str(np.mean(unt0['precision']))+'\\n' )\n",
    "  file.write('UNT 0 Recall:'+str(np.mean(unt0['recall']))+'\\n' )\n",
    "  file.write('UNT 0 F1-Score:'+str(np.mean(unt0['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('TIN 1 Precision:'+str(np.mean(tin1['precision']))+'\\n' )\n",
    "  file.write('TIN 1 Recall:'+str(np.mean(tin1['recall']))+'\\n' )\n",
    "  file.write('TIN 1 F1-Score:'+str(np.mean(tin1['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Weighted Avg Precision:'+str(np.mean(weightedavg['precision']))+'\\n' )\n",
    "  file.write('Weighted Avg Recall:'+str(np.mean(weightedavg['recall']))+'\\n' )\n",
    "  file.write('Weighted Avg F1-Score:'+str(np.mean(weightedavg['f1-score']))+'\\n' )\n",
    "\n",
    "  file.write('Macro  Precision:'+str(np.mean(macroavg['precision']))+'\\n' )\n",
    "  file.write('Macro  Recall:'+str(np.mean(macroavg['recall']))+'\\n' )\n",
    "  file.write('Macro  F1-Score:'+str(np.mean(macroavg['f1-score']))+'\\n' )\n",
    "  file.close()\n",
    "  print(\"Done\")\n",
    "\n",
    "######################################################################################\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bertmodelname)\n",
    "config = AutoConfig.from_pretrained(bertmodelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local System Time: 09:28 PM\n",
      "Fold1 Ep1/20 - ValMacroF1=0.5440 Pat=0\n",
      "Fold1 Ep2/20 - ValMacroF1=0.6403 Pat=0\n",
      "Fold1 Ep3/20 - ValMacroF1=0.7049 Pat=0\n",
      "Fold1 Ep4/20 - ValMacroF1=0.6833 Pat=1\n",
      "Fold1 Ep5/20 - ValMacroF1=0.7506 Pat=0\n",
      "Fold1 Ep6/20 - ValMacroF1=0.6668 Pat=1\n",
      "Fold1 Ep7/20 - ValMacroF1=0.6782 Pat=2\n",
      "Fold1 Ep8/20 - ValMacroF1=0.7364 Pat=3\n",
      "Fold1 Ep9/20 - ValMacroF1=0.7363 Pat=4\n",
      "Stopping early at epoch 9\n",
      "Completed fold 1/5\n",
      "Fold2 Ep1/20 - ValMacroF1=0.5466 Pat=0\n",
      "Fold2 Ep2/20 - ValMacroF1=0.6949 Pat=0\n",
      "Fold2 Ep3/20 - ValMacroF1=0.7445 Pat=0\n",
      "Fold2 Ep4/20 - ValMacroF1=0.7203 Pat=1\n",
      "Fold2 Ep5/20 - ValMacroF1=0.6955 Pat=2\n",
      "Fold2 Ep6/20 - ValMacroF1=0.7125 Pat=3\n",
      "Fold2 Ep7/20 - ValMacroF1=0.7135 Pat=4\n",
      "Stopping early at epoch 7\n",
      "Completed fold 2/5\n",
      "Fold3 Ep1/20 - ValMacroF1=0.6941 Pat=0\n",
      "Fold3 Ep2/20 - ValMacroF1=0.7080 Pat=0\n",
      "Fold3 Ep3/20 - ValMacroF1=0.7382 Pat=0\n",
      "Fold3 Ep4/20 - ValMacroF1=0.7377 Pat=1\n",
      "Fold3 Ep5/20 - ValMacroF1=0.7378 Pat=2\n",
      "Fold3 Ep6/20 - ValMacroF1=0.7584 Pat=0\n",
      "Fold3 Ep7/20 - ValMacroF1=0.7407 Pat=1\n",
      "Fold3 Ep8/20 - ValMacroF1=0.7571 Pat=2\n",
      "Fold3 Ep9/20 - ValMacroF1=0.7546 Pat=3\n",
      "Fold3 Ep10/20 - ValMacroF1=0.7564 Pat=4\n",
      "Stopping early at epoch 10\n",
      "Completed fold 3/5\n",
      "Fold4 Ep1/20 - ValMacroF1=0.6634 Pat=0\n",
      "Fold4 Ep2/20 - ValMacroF1=0.7304 Pat=0\n",
      "Fold4 Ep3/20 - ValMacroF1=0.7345 Pat=0\n",
      "Fold4 Ep4/20 - ValMacroF1=0.7466 Pat=0\n",
      "Fold4 Ep5/20 - ValMacroF1=0.6944 Pat=1\n",
      "Fold4 Ep6/20 - ValMacroF1=0.7322 Pat=2\n",
      "Fold4 Ep7/20 - ValMacroF1=0.7365 Pat=3\n",
      "Fold4 Ep8/20 - ValMacroF1=0.7304 Pat=4\n",
      "Stopping early at epoch 8\n",
      "Completed fold 4/5\n",
      "Fold5 Ep1/20 - ValMacroF1=0.6020 Pat=0\n",
      "Fold5 Ep2/20 - ValMacroF1=0.6527 Pat=0\n",
      "Fold5 Ep3/20 - ValMacroF1=0.6988 Pat=0\n",
      "Fold5 Ep4/20 - ValMacroF1=0.6944 Pat=1\n",
      "Fold5 Ep5/20 - ValMacroF1=0.7139 Pat=0\n",
      "Fold5 Ep6/20 - ValMacroF1=0.6891 Pat=1\n",
      "Fold5 Ep7/20 - ValMacroF1=0.6927 Pat=2\n",
      "Fold5 Ep8/20 - ValMacroF1=0.7010 Pat=3\n",
      "Fold5 Ep9/20 - ValMacroF1=0.6964 Pat=4\n",
      "Stopping early at epoch 9\n",
      "Completed fold 5/5\n",
      "Total runtime: 0 hrs 24 mins 17.30 secs\n",
      "Accuracy: 0.8054363586104744\n",
      "\n",
      "UNT 0 Precision: 0.7553024476077831\n",
      "UNT 0 Recall: 0.3784738260200154\n",
      "UNT 0 F1-Score: 0.4630133374985064\n",
      "\n",
      "TIN 1 Precision: 0.82579371732956\n",
      "TIN 1 Recall: 0.9455624058783669\n",
      "TIN 1 F1-Score: 0.8797016953724569\n",
      "\n",
      "Weighted Avg Precision: 0.8083796635913947\n",
      "Weighted Avg Recall: 0.8054363586104744\n",
      "Weighted Avg F1-Score: 0.7767366488454863\n",
      "\n",
      "Macro  Precision: 0.7905480824686716\n",
      "Macro  Recall: 0.6620181159491911\n",
      "Macro  F1-Score: 0.6713575164354817\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# --------- K-Fold Training ----------------\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "# Metrics storage\n",
    "valaccuracy, valprecision, valrecall, valf1, valcm = [], [], [], [], []\n",
    "val_prec_pc, val_rec_pc, val_f1_pc, val_f1_macro = [], [], [], []\n",
    "testaccuracy, testprecision, testrecall, testf1, testcm = [], [], [], [], []\n",
    "test_prec_pc, test_rec_pc, test_f1_pc, test_f1_macro = [], [], [], []\n",
    "reports = []\n",
    "start = time.time()\n",
    "print(\"Local System Time:\", time.strftime(\"%I:%M %p\", time.localtime()))\n",
    "\n",
    "for fold, (tr, te) in enumerate(skf.split(df[xcolumn], df[ycolumn]),1):\n",
    "    xtr = df.loc[tr, xcolumn].tolist();\n",
    "    ytr = df.loc[tr, ycolumn].values\n",
    "    xte = df.loc[te, xcolumn].tolist(); \n",
    "    yte = df.loc[te, ycolumn].values\n",
    "    xtr, xv, ytr, yv = train_test_split(xtr, ytr, test_size=0.15, random_state=0)\n",
    "    # tokenize\n",
    "    enc_tr = tokenizer(xtr, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_v  = tokenizer(xv,  padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    enc_te = tokenizer(xte, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "    # datasets & loaders\n",
    "    dt_tr = TextDataset(enc_tr, ytr); \n",
    "    lt = DataLoader(dt_tr, batch_size=BATCH, shuffle=True)\n",
    "    dt_v  = TextDataset(enc_v,  yv); \n",
    "    lv = DataLoader(dt_v, batch_size=BATCH)\n",
    "    dt_te = TextDataset(enc_te, yte); \n",
    "    le = DataLoader(dt_te, batch_size=BATCH)\n",
    "    # model, loss, opt\n",
    "    model = BertClassifier(bertmodelname).to(device)\n",
    "    crit = nn.BCELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "    best_f, pt = -np.inf, 0\n",
    "    # train\n",
    "    for e in range(NEPOCHS):\n",
    "        model.train()\n",
    "        for b in lt:\n",
    "            opt.zero_grad()\n",
    "            ids = b['input_ids'].to(device); \n",
    "            m = b['attention_mask'].to(device)\n",
    "            lbls = b['labels'].to(device)\n",
    "            pr = model(ids,m)\n",
    "            loss = crit(pr, lbls)\n",
    "            loss.backward(); opt.step()\n",
    "        # val\n",
    "        model.eval(); vp, vt = [], []\n",
    "        with torch.no_grad():\n",
    "            for b in lv:\n",
    "                ids = b['input_ids'].to(device); \n",
    "                m = b['attention_mask'].to(device)\n",
    "                vp.extend(model(ids,m).cpu().numpy().tolist()); \n",
    "                vt.extend(b['labels'].cpu().numpy().tolist())\n",
    "        th = optimize_threshold(np.array(vt), np.array(vp))\n",
    "        vpred = (np.array(vp)>=th).astype(int)\n",
    "        # record\n",
    "        valaccuracy.append(accuracy_score(vt, vpred)); \n",
    "        valprecision.append(precision_score(vt, vpred))\n",
    "        valrecall.append(recall_score(vt, vpred)); \n",
    "        valf1.append(f1_score(vt, vpred))\n",
    "        valcm.append(confusion_matrix(vt, vpred))\n",
    "        # per-class\n",
    "       \n",
    "        vm = f1_score(vt, vpred, average='macro'); \n",
    "        val_f1_macro.append(vm)\n",
    "        # early stop & save\n",
    "        if vm>best_f:\n",
    "            best_f, pt = vm, 0\n",
    "            torch.save(model.state_dict(), os.path.join(modelpath,f\"{modelname}_fold{fold}.bin\"))\n",
    "        else:\n",
    "            pt+=1\n",
    "        if DECAY and pt%DECAY_AFTER==0 and pt!=0:\n",
    "            for g in opt.param_groups: g['lr']*=DECAY_RATE\n",
    "        print(f\"Fold{fold} Ep{e+1}/{NEPOCHS} - ValMacroF1={vm:.4f} Pat={pt}\")\n",
    "        if pt>=PATIENCE:\n",
    "            print(f\"Stopping early at epoch {e+1}\"); break\n",
    "    # test eval\n",
    "    model.load_state_dict(torch.load(os.path.join(modelpath,f\"{modelname}_fold{fold}.bin\")))\n",
    "    model.eval(); tp, tt = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in le:\n",
    "            ids = b['input_ids'].to(device); \n",
    "            m = b['attention_mask'].to(device)\n",
    "            tp.extend(model(ids,m).cpu().numpy().tolist()); \n",
    "            tt.extend(b['labels'].cpu().numpy().tolist())\n",
    "    tpred = (np.array(tp)>=th).astype(int)\n",
    "    testaccuracy.append(accuracy_score(tt,tpred)); \n",
    "    testprecision.append(precision_score(tt,tpred))\n",
    "    testrecall.append(recall_score(tt,tpred)); \n",
    "    testf1.append(f1_score(tt,tpred))\n",
    "    testcm.append(confusion_matrix(tt,tpred))\n",
    "    reports.append(classification_report( tt, tpred, output_dict=True, zero_division=0, target_names=['UNT 0', 'TIN 1']))\n",
    "    print(f\"Completed fold {fold}/5\")\n",
    "    # cleanup\n",
    "    del model; torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(f\"Total runtime: {hms_string(time.time()-start)}\")\n",
    "WriteResutls(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "055bc683a2e74bdeb392dd1957ccb1b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0626dee929b245f3a3060f36e04e56bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9cb13080f3e412f9993d3624beae077",
       "IPY_MODEL_6e1d2517eb044bcebd077ca6408f258a",
       "IPY_MODEL_55e64cf8425f46e783f9103899ee1cd1"
      ],
      "layout": "IPY_MODEL_055bc683a2e74bdeb392dd1957ccb1b6"
     }
    },
    "07cd1e6af8da4e519647110727ae65cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08ac0e9b1e00432d91c6749339937a43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4234ba8d5ff846dc8b9a0dac23e5c3b4",
       "IPY_MODEL_7ce13a71ee7a4289b03724abdb5be89d",
       "IPY_MODEL_cb04ab03187e4e06a42a7366bc953f8e"
      ],
      "layout": "IPY_MODEL_4169f5e47e464366841151514c72f7a3"
     }
    },
    "0999a9d5c20049a8b3ffe426c35bcf3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccbe4e6af97e45ac8019e9b958771187",
      "placeholder": "​",
      "style": "IPY_MODEL_a1120e87b34446109cd377964e2cfa45",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 31.8MB/s]"
     }
    },
    "1151e5a047f041f9ac4c8b830d9061f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1f6b9263d7584fc8896150e000b709c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_638805ab4f1b4f3da186465fbd36bf01",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_afbd6222b9b64c79a8ed95cf1ae954d4",
      "value": 25
     }
    },
    "24197ce2f45c41b5ae194469d2f9cd4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "263d73ba90a14edf8a4ba379627e9b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b28c9309beb47e5981a56d670207250": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ddf2f0fb85a4f46859bc50f48d382ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fe226f85c764d45a5bc63044f156763": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cc8e85ba79347d58a2c417f8d3c72a8",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07cd1e6af8da4e519647110727ae65cd",
      "value": 9096718
     }
    },
    "3855a0b5bca044b19f100401742dd964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd20a7cda9d54cbc9cc87b2a7c29d16f",
      "placeholder": "​",
      "style": "IPY_MODEL_2b28c9309beb47e5981a56d670207250",
      "value": " 25.0/25.0 [00:00&lt;00:00, 1.51kB/s]"
     }
    },
    "3870f9ff6f074f2c8aa4aefa9306a307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2faa613e35e4887975945574617ce02",
      "placeholder": "​",
      "style": "IPY_MODEL_2ddf2f0fb85a4f46859bc50f48d382ca",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4169f5e47e464366841151514c72f7a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4234ba8d5ff846dc8b9a0dac23e5c3b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c1b030b875474cabe57f162a65e175",
      "placeholder": "​",
      "style": "IPY_MODEL_7ef02693da264329aa36de97b45bb154",
      "value": "sentencepiece.bpe.model: 100%"
     }
    },
    "45c1b030b875474cabe57f162a65e175": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bb8a29050c24e73b3fb26eb31bcc80c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4eb0760d72924bef927cbc661bccbd5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55e64cf8425f46e783f9103899ee1cd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aeb3673b112e4620b8c712ab7979a97d",
      "placeholder": "​",
      "style": "IPY_MODEL_cb61fad0e7c6435997550ecc29a17e6c",
      "value": " 615/615 [00:00&lt;00:00, 17.8kB/s]"
     }
    },
    "638805ab4f1b4f3da186465fbd36bf01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64ffb2d3b5094507b087138dc56bac86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67b7c958c2f4452b8ed0d1bdfa31b1f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e1d2517eb044bcebd077ca6408f258a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67b7c958c2f4452b8ed0d1bdfa31b1f0",
      "max": 615,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffed429885e4429f95301cbdd6da0726",
      "value": 615
     }
    },
    "74c5a71c6d4047dd9c66e59ce4173750": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a31d867fb8e04d09997269a4a100f8f3",
       "IPY_MODEL_cafcff3b21c446369358c23a5d388aa9",
       "IPY_MODEL_d0a6dd425f104139a415302c31bc8d70"
      ],
      "layout": "IPY_MODEL_24197ce2f45c41b5ae194469d2f9cd4c"
     }
    },
    "7cc8e85ba79347d58a2c417f8d3c72a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ce13a71ee7a4289b03724abdb5be89d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9314836e1634b53a3c9f5b9d5c23329",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bb8a29050c24e73b3fb26eb31bcc80c",
      "value": 5069051
     }
    },
    "7ef02693da264329aa36de97b45bb154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8218a57782d7495394945cfbbf87213e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9704dc4aaa374c34982d9545fe9760ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1120e87b34446109cd377964e2cfa45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a31d867fb8e04d09997269a4a100f8f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8218a57782d7495394945cfbbf87213e",
      "placeholder": "​",
      "style": "IPY_MODEL_b5226f27887a4616bf2848a342812083",
      "value": "model.safetensors: 100%"
     }
    },
    "a3a8ae066d7f455f8a4fc481e78d89e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a693dc69f27f481db7efd455c1ebbeeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a81faf0a4a6a4834b9b91363a2ad6ba6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d69b704def054170a3f2ad44e3ef37ab",
       "IPY_MODEL_2fe226f85c764d45a5bc63044f156763",
       "IPY_MODEL_0999a9d5c20049a8b3ffe426c35bcf3b"
      ],
      "layout": "IPY_MODEL_bfd633b618ea4efb8a457d1f80cecd65"
     }
    },
    "aeb3673b112e4620b8c712ab7979a97d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afbd6222b9b64c79a8ed95cf1ae954d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5226f27887a4616bf2848a342812083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba43741fafda4a9b83023e784aa66e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3870f9ff6f074f2c8aa4aefa9306a307",
       "IPY_MODEL_1f6b9263d7584fc8896150e000b709c4",
       "IPY_MODEL_3855a0b5bca044b19f100401742dd964"
      ],
      "layout": "IPY_MODEL_64ffb2d3b5094507b087138dc56bac86"
     }
    },
    "bfd633b618ea4efb8a457d1f80cecd65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cafcff3b21c446369358c23a5d388aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd26b947dba3415c8245a053cef46110",
      "max": 1115567652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_263d73ba90a14edf8a4ba379627e9b2d",
      "value": 1115567652
     }
    },
    "cb04ab03187e4e06a42a7366bc953f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edbfb62bb6284c7cb0da1aebd851e04c",
      "placeholder": "​",
      "style": "IPY_MODEL_4eb0760d72924bef927cbc661bccbd5b",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 16.2MB/s]"
     }
    },
    "cb61fad0e7c6435997550ecc29a17e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccbe4e6af97e45ac8019e9b958771187": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd20a7cda9d54cbc9cc87b2a7c29d16f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0a6dd425f104139a415302c31bc8d70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e43ab3e305c140e2afe7e27c31b5d860",
      "placeholder": "​",
      "style": "IPY_MODEL_1151e5a047f041f9ac4c8b830d9061f2",
      "value": " 1.12G/1.12G [00:06&lt;00:00, 149MB/s]"
     }
    },
    "d26fcb3e4551450abe815a8ad757e91b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d69b704def054170a3f2ad44e3ef37ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d26fcb3e4551450abe815a8ad757e91b",
      "placeholder": "​",
      "style": "IPY_MODEL_9704dc4aaa374c34982d9545fe9760ea",
      "value": "tokenizer.json: 100%"
     }
    },
    "d9314836e1634b53a3c9f5b9d5c23329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2faa613e35e4887975945574617ce02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43ab3e305c140e2afe7e27c31b5d860": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edbfb62bb6284c7cb0da1aebd851e04c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9cb13080f3e412f9993d3624beae077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3a8ae066d7f455f8a4fc481e78d89e2",
      "placeholder": "​",
      "style": "IPY_MODEL_a693dc69f27f481db7efd455c1ebbeeb",
      "value": "config.json: 100%"
     }
    },
    "fd26b947dba3415c8245a053cef46110": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffed429885e4429f95301cbdd6da0726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
